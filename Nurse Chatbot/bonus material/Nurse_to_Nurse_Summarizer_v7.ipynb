{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Improvements**\n",
    "1. Information Retrieval\n",
    "2. More Accurate Generated Answers\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imports\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, scrolledtext\n",
    "import os\n",
    "import threading \n",
    "import time\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Text, ForeignKey\n",
    "from sqlalchemy.orm import sessionmaker, relationship, declarative_base\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import atexit\n",
    "import json\n",
    "\n",
    "# Imports handle LLM workflow\n",
    "from langchain_community.document_loaders import JSONLoader, PyPDFLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, RecursiveJsonSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables import RunnableLambda, RunnableWithMessageHistory\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.retrievers import MergerRetriever\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chat History Database setup\n",
    "DATABASE_URL = \"sqlite:///Nurse_Chat_History_2.db\"\n",
    "Base = declarative_base()\n",
    "\n",
    "class Session(Base):\n",
    "    __tablename__ = \"sessions\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    session_id = Column(String, unique=True, nullable=False)\n",
    "    messages = relationship(\"Message\", back_populates=\"session\")\n",
    "\n",
    "class Message(Base):\n",
    "    __tablename__ = \"messages\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    session_id = Column(Integer, ForeignKey(\"sessions.id\"), nullable=False)\n",
    "    role = Column(String, nullable=False)\n",
    "    content = Column(Text, nullable=False)\n",
    "    session = relationship(\"Session\", back_populates=\"messages\")\n",
    "\n",
    "# Create the database and the tables\n",
    "engine = create_engine(DATABASE_URL)\n",
    "Base.metadata.create_all(engine)\n",
    "SessionLocal = sessionmaker(bind=engine)\n",
    "\n",
    "def get_db():\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save a single message\n",
    "def save_message(session_id: str, role: str, content: str):\n",
    "    with SessionLocal() as db:\n",
    "        try:\n",
    "            session = db.query(Session).filter(Session.session_id == session_id).first()\n",
    "            if not session:\n",
    "                session = Session(session_id=session_id)\n",
    "                db.add(session)\n",
    "                db.commit()\n",
    "                db.refresh(session)\n",
    "\n",
    "            db.add(Message(session_id=session.id, role=role, content=content))\n",
    "            db.commit()\n",
    "        except SQLAlchemyError:\n",
    "            db.rollback()\n",
    "\n",
    "\n",
    "# Function to load chat history\n",
    "def load_session_history(session_id: str) -> ChatMessageHistory:\n",
    "    chat_history = ChatMessageHistory()\n",
    "    with SessionLocal() as db:\n",
    "        try:\n",
    "            session = db.query(Session).filter(Session.session_id == session_id).first()\n",
    "            if session:\n",
    "                for message in session.messages:\n",
    "                    chat_history.add_message({\"role\": message.role, \"content\": message.content})\n",
    "        except SQLAlchemyError:\n",
    "            pass\n",
    "\n",
    "    return chat_history\n",
    "\n",
    "\n",
    "# Modify the get_session_history function to use the database\n",
    "def get_session_history(session_id: str) -> ChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        # Load from the database if not in store\n",
    "        store[session_id] = load_session_history(session_id)\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "# Ensure you save the chat history to the database when needed\n",
    "def save_all_sessions():\n",
    "    for session_id, chat_history in store.items():\n",
    "        for message in chat_history.messages:\n",
    "            save_message(session_id, message[\"role\"], message[\"content\"])\n",
    "\n",
    "# Register function to save sessions before exit\n",
    "atexit.register(save_all_sessions)\n",
    "\n",
    "store = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nurse2NurseChatbotSummarize(filepath: str, session_id: str, promptQuestion: str):\n",
    "    try:\n",
    "        # Initialize LLM\n",
    "        llm = ChatOpenAI(model='gpt-4o')\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Patient Data Pipeline (JSON)\n",
    "        # -----------------------------\n",
    "        # Load JSON patient data and split into chunks\n",
    "        loader = JSONLoader(file_path=filepath, jq_schema='.', text_content=False)\n",
    "        docs = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=200, add_start_index=True)\n",
    "        patient_splits = text_splitter.split_documents(docs)\n",
    "        \n",
    "        # Create vectorstore and retriever for patient data\n",
    "        patient_vectorstore = InMemoryVectorStore.from_documents(documents=patient_splits, embedding=OpenAIEmbeddings())\n",
    "        patient_retriever = patient_vectorstore.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
    "        \n",
    "        # Create a history-aware retriever for patient data (so that chat history can be integrated)\n",
    "        contextualize_q_system_prompt = (\n",
    "            \"Given a chat history and the latest user question \"\n",
    "            \"which might reference context in the chat history, \"\n",
    "            \"formulate a standalone question that can be understood without the chat history. \"\n",
    "            \"Do NOT answer the question; just reformulate it if needed.\"\n",
    "        )\n",
    "        contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", contextualize_q_system_prompt),\n",
    "                MessagesPlaceholder(\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "        history_aware_retriever = create_history_aware_retriever(llm, patient_retriever, contextualize_q_prompt)\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Vital Signs Pipeline (PDF)\n",
    "        # -----------------------------\n",
    "        # Load PDF (vital signs), split into chunks, embed, and create a retriever\n",
    "        pdf_loader = PyPDFLoader(file_path='NormalVitals.pdf')\n",
    "        pdf_docs = pdf_loader.load()\n",
    "        PDF_text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=0, add_start_index=True)\n",
    "        pdf_splits = PDF_text_splitter.split_documents(pdf_docs)\n",
    "        pdf_vectorstore = InMemoryVectorStore.from_documents(documents=pdf_splits, embedding=OpenAIEmbeddings())\n",
    "        pdf_retriever = pdf_vectorstore.as_retriever(search_type='similarity', search_kwargs={'k': 1})\n",
    "        \n",
    "        # --------------------------------------------\n",
    "        # Define System Prompt with Two Placeholders\n",
    "        # --------------------------------------------\n",
    "        system_prompt = (\n",
    "            \"You are a nurse in the NICU at the end of your shift, preparing for patient handover. \"\n",
    "            \"Provide the incoming nurse with all pertinent information for their shift in a concise, technical summary.\\n\\n\"\n",
    "            \n",
    "            \"For each vital sign in the patient data, compare its measured values against the reference ranges provided in {vitals}. \"\n",
    "\n",
    "            \"For each vital sign (e.g., heart rate and respiratory rate):\\n\"\n",
    "            \"  - If the measured value is below the lower bound, classify that instance as 'bradycardia' (for heart rate) or 'bradypnea' (for respiratory rate).\\n\"\n",
    "            \"  - If the measured value is above the upper bound, classify that instance as 'tachycardia' (for heart rate) or 'tachypnea' (for respiratory rate).\\n\"\n",
    "            \"  - If the measurements vary such that some readings are below and some are above, explicitly state that both abnormal conditions occurred.\\n\"\n",
    "            \"Only mention conditions that are fully supported by the patient data.\\n\\n\"\n",
    "            \n",
    "            \"Before finalizing your summary, perform the following internal check (do not output these steps):\\n\"\n",
    "            \"  1. List all the measured values for each vital sign from the patient data.\\n\"\n",
    "            \"  2. Compare each measured value to the corresponding lower and upper bounds from {vitals}.\\n\"\n",
    "            \"  3. Verify that the abnormal condition you are about to report is consistent with every measurement. \"\n",
    "            \"If any measurement contradicts the reported condition, state that the readings vary and provide the observed range without declaring an abnormal condition.\\n\\n\"\n",
    "            \n",
    "            \"Triple-check the reference ranges in {vitals} before stating the condition to ensure the comparison is accurate. \"\n",
    "            \n",
    "            \"The life of an innocent child is at stake. If you get the condition wrong, the child may die. SO BE SURE BEFORE STATING THE CONDITION.\\n\\n\"\n",
    "            \n",
    "            \"Include the observed range of the patient's vital sign measurements in your summary. For example, if the patient's heart rate is consistently recorded at 130 bpm, state: \"\n",
    "            \"'The patient's heart rate ranged from 130 bpm to 130 bpm, indicating bradycardia.' Do not include the numerical reference ranges from {vitals} in the final summary; use them solely for comparison.\\n\\n\"\n",
    "            \n",
    "            \"Example: If the reference heart rate range for a preterm is 141–171 bpm and the patient's heart rate is consistently 130 bpm, the summary should note that the heart rate ranged from 130 bpm to 130 bpm, indicating 'bradycardia'. \"\n",
    "            \"Similarly, if the reference respiratory rate range is 40–70 breaths/min and the patient's respiratory rate is consistently 29 breaths/min, the summary should note that the respiratory rate indicates 'bradypnea'. \"\n",
    "            \"If a vital sign has readings that vary, such that some are below and others above the normal range, state the observed range and mention that both conditions occurred.\\n\\n\"\n",
    "\n",
    "            \"IF prompted to list every occurrence of an abnormal event: List each occurrence with its corresponding timestamp and measured vital sign. \"\n",
    "            \"Example: Extract every instance where the patient's heart rate falls below the lower bound specified in the reference ranges in {vitals}, \"\n",
    "            \n",
    "            \"Include any interventions performed and describe how the vital signs varied. \"\n",
    "            \"Assume the colleague is a medical professional familiar with normal ranges, so do not repeat the reference ranges or specific numerical details from {vitals} in your summary. \"\n",
    "            \"Avoid diagnosing.\\n\\n\"\n",
    "            \n",
    "            \"Patient data:\"\n",
    "            \"\\n{context}\\n\\n\"\n",
    "            \n",
    "            \"Reference medical vital sign ranges:\"\n",
    "            \"\\n{vitals}\\n\\n\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        qa_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system_prompt),\n",
    "                MessagesPlaceholder(\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "        question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "        \n",
    "        # --------------------------------------------------\n",
    "        # Custom Lambda to Retrieve and Combine Both Contexts\n",
    "        # --------------------------------------------------\n",
    "        def retrieve_and_combine(input_dict):\n",
    "            input_val = input_dict.get(\"input\")\n",
    "            chat_history = input_dict.get(\"chat_history\", [])\n",
    "            if not input_val:\n",
    "                raise ValueError(\"Input question is missing.\")\n",
    "            \n",
    "            # Retrieve patient data context using the history-aware retriever\n",
    "            patient_context = history_aware_retriever.invoke({\"input\": input_val, \"chat_history\": chat_history})\n",
    "            \n",
    "            # Retrieve vital signs context using the PDF retriever.\n",
    "            # You might customize the query based on patient data (e.g., age) if needed.\n",
    "            vitals_query = \"Normal Heart Rate and Respiratory rate Ranges and Normal Temperature Ranges for preterm child\"\n",
    "            vitals_docs = pdf_retriever.get_relevant_documents(vitals_query)\n",
    "            vitals_context = \"\\n\".join([doc.page_content for doc in vitals_docs])\n",
    "            \n",
    "            return {\n",
    "                \"context\": patient_context,  # Patient data context\n",
    "                \"vitals\": vitals_context,      # Vital signs context\n",
    "                \"input\": input_val,\n",
    "                \"chat_history\": chat_history\n",
    "            }\n",
    "        \n",
    "        # Chain the custom lambda with the QA chain.\n",
    "        custom_chain = RunnableLambda(retrieve_and_combine) | question_answer_chain\n",
    "        wrapped_chain = custom_chain | RunnableLambda(lambda output: {\"answer\": output})\n",
    "        \n",
    "        # Wrap with RunnableWithMessageHistory to incorporate chat history\n",
    "        conversational_rag_chain = RunnableWithMessageHistory(\n",
    "            wrapped_chain,\n",
    "            get_session_history,  # Assumes this is defined elsewhere\n",
    "            input_messages_key=\"input\",\n",
    "            history_messages_key=\"chat_history\",\n",
    "            output_messages_key=\"answer\",\n",
    "        )\n",
    "        \n",
    "        # Invoke the chain\n",
    "        response = conversational_rag_chain.invoke(\n",
    "            {\"input\": promptQuestion},\n",
    "            config={\"configurable\": {\"session_id\": session_id}},\n",
    "        )\n",
    "        \n",
    "        # Save messages to the database\n",
    "        save_message(session_id, \"human\", promptQuestion)\n",
    "        save_message(session_id, \"ai\", response['answer'])\n",
    "        \n",
    "        return response['answer']\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, scrolledtext\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "from tkinterdnd2 import TkinterDnD, DND_FILES\n",
    "\n",
    "# Global loading flag\n",
    "loading = False\n",
    "\n",
    "# Colors and Styles\n",
    "BG_COLOR = \"#2C2C2C\"\n",
    "FG_COLOR = \"#FFFFFF\"\n",
    "BTN_COLOR = \"#4CAF50\"\n",
    "ENTRY_COLOR = \"#3E3E3E\"\n",
    "FONT = (\"Arial\", 12)\n",
    "\n",
    "# Loading Dots Animation\n",
    "def loading_dots():\n",
    "    dot_count = 0\n",
    "    while loading:\n",
    "        dot_count = (dot_count % 3) + 1\n",
    "        dots = \". \" * dot_count\n",
    "        chat_history_text.configure(state='normal')\n",
    "        chat_history_text.delete('end-1c linestart', 'end-1c')  # Remove previous dots\n",
    "        chat_history_text.insert(tk.END, dots)\n",
    "        chat_history_text.configure(state='disabled')\n",
    "        root.update_idletasks()\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Run Chatbot Function\n",
    "def run_chatbot():\n",
    "    filepath = file_entry.get()\n",
    "    session_id = session_entry.get()\n",
    "    prompt = prompt_entry.get()\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        messagebox.showerror(\"Error\", \"The selected file does not exist.\")\n",
    "        return\n",
    "\n",
    "    if not session_id:\n",
    "        messagebox.showerror(\"Error\", \"Please enter a valid session ID.\")\n",
    "        return\n",
    "\n",
    "    if not prompt:\n",
    "        messagebox.showerror(\"Error\", \"Please enter a prompt.\")\n",
    "        return\n",
    "\n",
    "    chat_history_text.configure(state='normal')\n",
    "    chat_history_text.insert(tk.END, f\"You: {prompt}\\nBot: \")\n",
    "    chat_history_text.configure(state='disabled')\n",
    "    root.update_idletasks()\n",
    "\n",
    "    global loading\n",
    "    loading = True\n",
    "    threading.Thread(target=loading_dots, daemon=True).start()\n",
    "\n",
    "    def generate_response():\n",
    "        global loading\n",
    "        result = Nurse2NurseChatbotSummarize(filepath, session_id, prompt)  # Placeholder for actual function\n",
    "        loading = False\n",
    "        chat_history_text.configure(state='normal')\n",
    "        chat_history_text.delete('end-1c linestart', 'end-1c')  # Remove loading dots\n",
    "        chat_history_text.insert(tk.END, f\"{result}\\n\\n\")\n",
    "        chat_history_text.configure(state='disabled')\n",
    "        prompt_entry.delete(0, tk.END)\n",
    "\n",
    "    threading.Thread(target=generate_response, daemon=True).start()\n",
    "\n",
    "# End Chat Function\n",
    "def end_chat():\n",
    "    chat_history_text.configure(state='normal')\n",
    "    chat_history_text.insert(tk.END, \"Chat session ended.\\n\\n\")\n",
    "    chat_history_text.configure(state='disabled')\n",
    "    session_entry.delete(0, tk.END)\n",
    "    prompt_entry.delete(0, tk.END)\n",
    "    file_entry.delete(0, tk.END)\n",
    "    root.after(1000, root.destroy)  # Close the GUI after 1 second\n",
    "\n",
    "# Drag-and-Drop Handler\n",
    "def on_file_drop(event):\n",
    "    file_entry.delete(0, tk.END)  # Clear existing text\n",
    "    file_entry.insert(0, event.data.strip())  # Insert the dropped file path\n",
    "\n",
    "# GUI Setup using TkinterDnD\n",
    "root = TkinterDnD.Tk()\n",
    "root.title(\"Chatbot GUI for JSON Summarization\")\n",
    "root.geometry(\"800x600\")\n",
    "root.configure(bg=BG_COLOR)\n",
    "\n",
    "# Header\n",
    "header = tk.Label(root, text=\"Chatbot Assistant\", font=(\"Arial\", 18, \"bold\"), bg=BG_COLOR, fg=FG_COLOR, pady=10)\n",
    "header.pack(fill=\"x\")\n",
    "\n",
    "# Filepath Entry Section\n",
    "frame_inputs = tk.Frame(root, bg=BG_COLOR)\n",
    "frame_inputs.pack(pady=10)\n",
    "\n",
    "file_label = tk.Label(frame_inputs, text=\"Select JSON File:\", font=FONT, bg=BG_COLOR, fg=FG_COLOR)\n",
    "file_label.grid(row=0, column=0, padx=5, pady=5, sticky=\"e\")\n",
    "\n",
    "file_entry = tk.Entry(frame_inputs, font=FONT, bg=ENTRY_COLOR, fg=FG_COLOR, bd=1, relief=\"solid\")\n",
    "file_entry.grid(row=0, column=1, padx=5, pady=5)\n",
    "\n",
    "# Enable drag-and-drop for file entry\n",
    "file_entry.drop_target_register(DND_FILES)\n",
    "file_entry.dnd_bind('<<Drop>>', on_file_drop)\n",
    "\n",
    "file_button = tk.Button(frame_inputs, text=\"Browse\", command=lambda: file_entry.insert(0, filedialog.askopenfilename(filetypes=[(\"JSON Files\", \"*.json\")])), bg=BTN_COLOR, fg=FG_COLOR, font=FONT, relief=\"flat\", padx=10)\n",
    "file_button.grid(row=0, column=2, padx=5, pady=5)\n",
    "\n",
    "# Session ID Entry\n",
    "session_label = tk.Label(frame_inputs, text=\"Enter Session ID:\", font=FONT, bg=BG_COLOR, fg=FG_COLOR)\n",
    "session_label.grid(row=1, column=0, padx=5, pady=5, sticky=\"e\")\n",
    "\n",
    "session_entry = tk.Entry(frame_inputs, font=FONT, bg=ENTRY_COLOR, fg=FG_COLOR, bd=1, relief=\"solid\")\n",
    "session_entry.grid(row=1, column=1, padx=5, pady=5)\n",
    "\n",
    "# Prompt Entry\n",
    "prompt_label = tk.Label(frame_inputs, text=\"Enter Prompt:\", font=FONT, bg=BG_COLOR, fg=FG_COLOR)\n",
    "prompt_label.grid(row=2, column=0, padx=5, pady=5, sticky=\"e\")\n",
    "\n",
    "prompt_entry = tk.Entry(frame_inputs, font=FONT, bg=ENTRY_COLOR, fg=FG_COLOR, bd=1, relief=\"solid\")\n",
    "prompt_entry.grid(row=2, column=1, padx=5, pady=5)\n",
    "\n",
    "# Chat History Display\n",
    "chat_history_text = scrolledtext.ScrolledText(root, width=80, height=20, state='disabled', wrap='word', font=FONT, bg=ENTRY_COLOR, fg=FG_COLOR, bd=1, relief=\"solid\")\n",
    "chat_history_text.pack(pady=10)\n",
    "\n",
    "# Buttons Section\n",
    "frame_buttons = tk.Frame(root, bg=BG_COLOR)\n",
    "frame_buttons.pack(pady=10)\n",
    "\n",
    "run_button = tk.Button(frame_buttons, text=\"Run Chatbot\", command=run_chatbot, bg=BTN_COLOR, fg=FG_COLOR, font=FONT, relief=\"flat\", padx=20)\n",
    "run_button.grid(row=0, column=0, padx=10)\n",
    "\n",
    "end_button = tk.Button(frame_buttons, text=\"End Chat\", command=end_chat, bg=\"#FF6347\", fg=FG_COLOR, font=FONT, relief=\"flat\", padx=20)\n",
    "end_button.grid(row=0, column=1, padx=10)\n",
    "\n",
    "# Mainloop to run the GUI\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
